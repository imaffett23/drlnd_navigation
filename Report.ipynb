{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Learning Algorithm\n",
    "\n",
    "The model architecture used for the neural network in this DQN is quite simple.  It has three dense layers which pass through ReLU activation functions to introduce non-linearity.  The shape that is input to the neural network corresponds to the environments state size, while that output corresponds to the number of actions.\n",
    "\n",
    "The agent itself is the Deep Q-Network (DQN) algorithm. DQN uses a replay buffer to store experiences and samples batches of these experiences to learn from over time.  Two identical neural networks are used in a DQN.  The online network estimates Q-values and updates at every time step.  The target network by comparison, is used to generate target Q-values that are used TD learning.  The target network is updated less frequently to avoid harmful correlations between networks that could lead to issues when calculating Q-values.\n",
    "\n",
    "This agent runs for a maximum of 2,000 episodes with a max of 1,000 timesteps alloted per episode.  Epsilon (for the epislon-greedy approach) starts at one with a decay factor of .995.  Epsilon won't drop below 0.01.  The agent will automatically stop training when the average score of the past 100 episodes has exceeded 13.0.  Model weights will then be saved out to `model.pt`.  A selection of relevant hyperparameters are given below:\n",
    "\n",
    "- Batch Size (64) - used to sample a more manageable set of samples from memory\n",
    "- Update Every (4) - controls the number of time steps we wait learn from experiences and soft update our target Q-network based on the online network.\n",
    "- Gamma (.99) - this is the discount factor. .99 indicates the importance of future rewards in decision making, but still giving some weight to immediate rewards as well.\n",
    "- Learning Rate (.0005) - this is the learning rate used for the Adam optimizer used in our neural networks.  It is relatively low and indicates a willingness to take a bit longer in training to guarantee convergence to an optimal solution.\n",
    "- Buffer Size (10,000)  - the maximum amount of experiences that will be held in the replay buffer.\n",
    "\n",
    "These hyperparameters - and others - could likely be tuned to reach better performance, but this agent was able to solve the problem in 389 episodes as such!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plot of Rewards\n",
    "\n",
    "A plot of rewards per episode was included in `Navigation.ipynb`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ideas for Future Work\n",
    "\n",
    "This model could likely be improved by extending the traditional DQN architecture to that of [Rainbow](https://arxiv.org/abs/1710.02298).  Rainbow features six extensions on the original DQN algorithm that was implemented for this project.  They include:\n",
    "\n",
    "- Double DQN (DDQN)\n",
    "- Prioritized Experience Replay\n",
    "- Dueling DQNs\n",
    "- Learning from multi-step bootstrap agents\n",
    "- Distributional DQNs\n",
    "- Noisy DQN\n",
    "\n",
    "Combining these six extensions under Rainbow addresses many of the shortcomings faced with using a traditional DQN and would likely lead to tangible improvements in performance!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
